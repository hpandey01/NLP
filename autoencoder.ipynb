{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "autoencoder.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-YvWLT1bUwL"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import time\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GowrvCzbeGk",
        "outputId": "51ff4f91-1758-4b2a-83f6-4a58e47032c5"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEjMWunQbfjl",
        "outputId": "a1258742-abc9-4462-cb6f-25da2b7b1347"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkmBaLJEwujg"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/train/training.1600000.processed.noemoticon.csv\", error_bad_lines=  False, encoding= 'latin-1', header= None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EhX7Os2313L"
      },
      "source": [
        "eng = []\n",
        "for i in range(10000):\n",
        "  eng += [df.at[i, 5]]eng = []\n",
        "\n",
        "for i in data:\n",
        "  eng += [i[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKdDkSzGwuvw"
      },
      "source": [
        "for i in range(len(eng)):\n",
        "  x = re.sub(r'[^a-zA-Z ]+', '', eng[i])\n",
        "  x = x.lower()\n",
        "  x = x.split(\" \")\n",
        "  x = [i for i in x if len(i)>0]\n",
        "  x = ' '.join(x)\n",
        "  eng[i] = x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jka736lobs4S",
        "outputId": "6934fa1e-9f22-4ba6-db78-a4d28733caa5"
      },
      "source": [
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'indic_nlp_library' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBbmFPhybu3b",
        "outputId": "2a8d81c0-64b8-4b54-d0c2-12498bc29e92"
      },
      "source": [
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'indic_nlp_resources' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VixufYxVbydW",
        "outputId": "51893941-ba34-4d09-fbfb-b77a480d2df5"
      },
      "source": [
        "!pip install Morfessor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Morfessor in /usr/local/lib/python3.7/dist-packages (2.0.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVmcM3dBbzzB"
      },
      "source": [
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\"\n",
        "\n",
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
        "\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "\n",
        "from indicnlp import loader\n",
        "loader.load()\n",
        "\n",
        "from indicnlp.normalize.indic_normalize import BaseNormalizer\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "normalizer = BaseNormalizer(\"hi\", remove_nuktas=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJg0Selab2ba"
      },
      "source": [
        "eng2tok={'<sos>':0, '<eos>':1}\n",
        "tok2eng={0:'<sos>', 1:'<eos>'}\n",
        "cnt_eng=2\n",
        "def addwordeng(word):\n",
        "  global cnt_eng, eng2tok, tok2eng;\n",
        "  if word not in eng2tok:\n",
        "    eng2tok[word]=cnt_eng\n",
        "    tok2eng[cnt_eng]=word\n",
        "    cnt_eng+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E62ucFCQb4wA"
      },
      "source": [
        "src=[]\n",
        "for i in range(1, len(eng)):\n",
        "  x = indic_tokenize.trivial_tokenize(eng[i])\n",
        "  for j in x:\n",
        "    addwordeng(j)\n",
        "  x.insert(0, '<sos>')\n",
        "  x.append('<eos>')\n",
        "  src += [x]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZtR_CxZb6Mq"
      },
      "source": [
        "for i in range(0,len(src)):\n",
        "  if len(src[i])>25:\n",
        "    src[i]=src[i][:24]\n",
        "    src[i].append('<eos>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAL393wkb_cy"
      },
      "source": [
        "# tokenized text is numericalized7.  You can cite papers like this [1, 2].  You can create a footnote like this\n",
        "src_tok=[]\n",
        "for i in src:\n",
        "  src_tok+=[[eng2tok[j] for j in i]]\n",
        "\n",
        "trg_tok=[]\n",
        "for i in src:\n",
        "  trg_tok+=[[eng2tok[j] for j in i]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e01jsHA9cFK-"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, hidden_dim, embedding_dim):\n",
        "    super().__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.embedding = nn.Embedding(input_size, embedding_dim)\n",
        "    self.GRU = nn.GRU(embedding_dim, hidden_dim)\n",
        "\n",
        "  def forward(self, input_tensor, hidden):\n",
        "    output_tensor = self.embedding(input_tensor).view(1, 1, -1)\n",
        "    output_tensor, hidden = self.GRU(output_tensor, hidden)\n",
        "    return output_tensor, hidden\n",
        "\n",
        "  def initHidden(self):\n",
        "    x = torch.zeros(1, 1, self.hidden_dim, device=device)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEpwNBopcGsb"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_size, hidden_dim, embedding_dim):\n",
        "    super().__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.embedding = nn.Embedding(output_size, embedding_dim)\n",
        "    self.GRU = nn.GRU(embedding_dim, hidden_dim)\n",
        "    self.out = nn.Linear(hidden_dim, output_size)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self, input_tensor, hidden):\n",
        "    output_tensor = self.embedding(input_tensor).view(1, 1, -1)\n",
        "    output_tensor = F.relu(output_tensor)\n",
        "    output_tensor, hidden = self.GRU(output_tensor, hidden)\n",
        "    output_tensor = self.out(output_tensor[0])\n",
        "    output_tensor = self.softmax(output_tensor)\n",
        "    return output_tensor, hidden\n",
        "\n",
        "  def initHidden(self):\n",
        "    x = torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2hgAUnRcIbR"
      },
      "source": [
        "def train(encoder, encoder_optimizer, decoder, decoder_optimizer, criteria, src, trg):\n",
        "  encoder_optimizer.zero_grad()\n",
        "  decoder_optimizer.zero_grad()\n",
        "\n",
        "  encoder_hidden = encoder.initHidden()\n",
        "\n",
        "  loss = 0\n",
        "  teacher_forcing = 0.4\n",
        "\n",
        "  for i in range(src.size(0)):   # iterate the input length\n",
        "    encoder_output, encoder_hidden = encoder(src[i], encoder_hidden)\n",
        "\n",
        "  decoder_input = torch.tensor([[0]], device=device)\n",
        "  decoder_hidden = encoder_hidden\n",
        "\n",
        "  if random.random() < teacher_forcing:\n",
        "    for i in range(trg.size(0)):  # iterate the input length\n",
        "      decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "      loss += criteria(decoder_output, trg[i])\n",
        "      decoder_input = trg[i]  # Teacher forcing\n",
        "\n",
        "  else:\n",
        "    for i in range(trg.size(0)):  # iterate the input length\n",
        "      decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "      topv, topi = decoder_output.topk(1)\n",
        "      decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "      loss += criteria(decoder_output, trg[i])\n",
        "      if decoder_input.item() == 1: # if EOS token is encountered\n",
        "        break\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  encoder_optimizer.step()\n",
        "  decoder_optimizer.step()\n",
        "\n",
        "  return loss.item() / trg.size(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aymCdY2kW7m8"
      },
      "source": [
        "# run this cell if you want to continue traing the model\n",
        "\n",
        "# encoder = Encoder(46120, 512, 256).to(device) # (dict_size, hidden_dim, embedding_dim)\n",
        "# decoder = Decoder(37706, 512, 256).to(device)\n",
        "\n",
        "# checkpoint = torch.load('/content/drive/MyDrive/parameter.pt')\n",
        "# encoder.load_state_dict(checkpoint['modelA_state_dict'])\n",
        "# decoder.load_state_dict(checkpoint['modelB_state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txBg8_MtcLNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae34d3e5-1219-49c4-8849-3ac12e0f4677"
      },
      "source": [
        "# uncomment the encoder and decoder to initialize the encoder-decoder architecture and start training from scratch\n",
        "\n",
        "encoder = Encoder(17120, 64, 64).to(device) # (dict_size, hidden_dim, embedding_dim)\n",
        "decoder = Decoder(17120, 64, 64).to(device)\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "epoch = 10       # total = 5\n",
        "\n",
        "start = time.time()\n",
        "total_loss = 0\n",
        "glob_loss = 0\n",
        "num_iters = 9999 * epoch\n",
        "batch =500\n",
        "\n",
        "encoder_optim = optim.SGD(encoder.parameters(), lr=0.01)\n",
        "decoder_optim = optim.SGD(decoder.parameters(), lr=0.01)\n",
        "\n",
        "criteria = nn.NLLLoss()\n",
        "\n",
        "input = [torch.tensor(i, dtype=torch.long, device=device).view(-1, 1) for i in src_tok]\n",
        "target = [torch.tensor(i, dtype=torch.long, device=device).view(-1, 1) for i in trg_tok]\n",
        "\n",
        "for i in range(1, num_iters + 1):\n",
        "  src = input[(i-1)%9999]\n",
        "  trg = target[(i-1)%9999]\n",
        "\n",
        "  loss = train(encoder, encoder_optim, decoder, decoder_optim, criteria, src, trg)\n",
        "  total_loss += loss\n",
        "  glob_loss  += loss\n",
        "\n",
        "  if i % batch == 0:\n",
        "    avg_loss= round(total_loss / batch, 4)\n",
        "    total_loss = 0\n",
        "    end = time.time()\n",
        "    mins = int((end - start) / 60)\n",
        "    secs = int((end-start) % 60)\n",
        "    time_taken = str(mins) + \"m \" + str(secs) + \"s\"\n",
        "    curr_epoch = (i // 9999) + 1\n",
        "    curr_batch = ((i % 9999) // batch)\n",
        "    print(\"time: \" + time_taken + \" Epoch:\" + str(curr_epoch) + \" Batch:\" + str(curr_batch) + \"/\" + str(9999//batch) + \" Loss:\" + str(avg_loss))\n",
        "\n",
        "  if i % 9999 == 0:\n",
        "    print(\"Epoch Completed. Epoch Loss:\" + str(round(glob_loss/ 9999, 4)))\n",
        "    print(\"-----------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "    glob_loss = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 0m 15s Epoch:1 Batch:1/19 Loss:4.7365\n",
            "time: 0m 30s Epoch:1 Batch:2/19 Loss:4.5906\n",
            "time: 0m 46s Epoch:1 Batch:3/19 Loss:4.6462\n",
            "time: 1m 1s Epoch:1 Batch:4/19 Loss:4.3857\n",
            "time: 1m 16s Epoch:1 Batch:5/19 Loss:4.4298\n",
            "time: 1m 32s Epoch:1 Batch:6/19 Loss:4.5854\n",
            "time: 1m 47s Epoch:1 Batch:7/19 Loss:4.4342\n",
            "time: 2m 3s Epoch:1 Batch:8/19 Loss:4.491\n",
            "time: 2m 19s Epoch:1 Batch:9/19 Loss:4.4306\n",
            "time: 2m 34s Epoch:1 Batch:10/19 Loss:4.3927\n",
            "time: 2m 50s Epoch:1 Batch:11/19 Loss:4.5464\n",
            "time: 3m 6s Epoch:1 Batch:12/19 Loss:4.6049\n",
            "time: 3m 21s Epoch:1 Batch:13/19 Loss:4.7085\n",
            "time: 3m 39s Epoch:1 Batch:14/19 Loss:5.0303\n",
            "time: 3m 56s Epoch:1 Batch:15/19 Loss:4.9933\n",
            "time: 4m 14s Epoch:1 Batch:16/19 Loss:4.999\n",
            "time: 4m 31s Epoch:1 Batch:17/19 Loss:5.0377\n",
            "time: 4m 49s Epoch:1 Batch:18/19 Loss:5.0493\n",
            "time: 5m 7s Epoch:1 Batch:19/19 Loss:5.076\n",
            "Epoch Completed. Epoch Loss:4.7094\n",
            "-----------------------------------------------------------------------------------------------------------------------------------------\n",
            "time: 5m 24s Epoch:2 Batch:0/19 Loss:5.0235\n",
            "time: 5m 43s Epoch:2 Batch:1/19 Loss:5.0709\n",
            "time: 6m 0s Epoch:2 Batch:2/19 Loss:5.0061\n",
            "time: 6m 18s Epoch:2 Batch:3/19 Loss:5.1931\n",
            "time: 6m 36s Epoch:2 Batch:4/19 Loss:5.192\n",
            "time: 6m 55s Epoch:2 Batch:5/19 Loss:5.206\n",
            "time: 7m 13s Epoch:2 Batch:6/19 Loss:5.2515\n",
            "time: 7m 32s Epoch:2 Batch:7/19 Loss:5.2486\n",
            "time: 7m 51s Epoch:2 Batch:8/19 Loss:5.2455\n",
            "time: 8m 10s Epoch:2 Batch:9/19 Loss:5.2075\n",
            "time: 8m 28s Epoch:2 Batch:10/19 Loss:5.175\n",
            "time: 8m 47s Epoch:2 Batch:11/19 Loss:5.1564\n",
            "time: 9m 5s Epoch:2 Batch:12/19 Loss:5.118\n",
            "time: 9m 22s Epoch:2 Batch:13/19 Loss:5.1437\n",
            "time: 9m 41s Epoch:2 Batch:14/19 Loss:5.1273\n",
            "time: 9m 59s Epoch:2 Batch:15/19 Loss:5.1362\n",
            "time: 10m 17s Epoch:2 Batch:16/19 Loss:5.1091\n",
            "time: 10m 35s Epoch:2 Batch:17/19 Loss:5.1528\n",
            "time: 10m 54s Epoch:2 Batch:18/19 Loss:5.1526\n",
            "time: 11m 13s Epoch:2 Batch:19/19 Loss:5.1796\n",
            "Epoch Completed. Epoch Loss:5.1602\n",
            "-----------------------------------------------------------------------------------------------------------------------------------------\n",
            "time: 11m 31s Epoch:3 Batch:0/19 Loss:5.1312\n",
            "time: 11m 50s Epoch:3 Batch:1/19 Loss:5.031\n",
            "time: 12m 8s Epoch:3 Batch:2/19 Loss:5.0552\n",
            "time: 12m 26s Epoch:3 Batch:3/19 Loss:5.1331\n",
            "time: 12m 45s Epoch:3 Batch:4/19 Loss:5.1437\n",
            "time: 13m 3s Epoch:3 Batch:5/19 Loss:5.0808\n",
            "time: 13m 22s Epoch:3 Batch:6/19 Loss:5.2134\n",
            "time: 13m 41s Epoch:3 Batch:7/19 Loss:5.1929\n",
            "time: 14m 0s Epoch:3 Batch:8/19 Loss:5.1192\n",
            "time: 14m 19s Epoch:3 Batch:9/19 Loss:5.1036\n",
            "time: 14m 38s Epoch:3 Batch:10/19 Loss:5.0474\n",
            "time: 14m 57s Epoch:3 Batch:11/19 Loss:5.0227\n",
            "time: 15m 15s Epoch:3 Batch:12/19 Loss:4.9697\n",
            "time: 15m 33s Epoch:3 Batch:13/19 Loss:5.0296\n",
            "time: 15m 51s Epoch:3 Batch:14/19 Loss:4.951\n",
            "time: 16m 10s Epoch:3 Batch:15/19 Loss:4.9119\n",
            "time: 16m 28s Epoch:3 Batch:16/19 Loss:5.0211\n",
            "time: 16m 47s Epoch:3 Batch:17/19 Loss:4.9685\n",
            "time: 17m 5s Epoch:3 Batch:18/19 Loss:4.9361\n",
            "time: 17m 24s Epoch:3 Batch:19/19 Loss:4.9945\n",
            "Epoch Completed. Epoch Loss:5.0415\n",
            "-----------------------------------------------------------------------------------------------------------------------------------------\n",
            "time: 17m 43s Epoch:4 Batch:0/19 Loss:4.9053\n",
            "time: 18m 2s Epoch:4 Batch:1/19 Loss:4.9021\n",
            "time: 18m 20s Epoch:4 Batch:2/19 Loss:4.9248\n",
            "time: 18m 39s Epoch:4 Batch:3/19 Loss:5.0175\n",
            "time: 18m 57s Epoch:4 Batch:4/19 Loss:4.8823\n",
            "time: 19m 16s Epoch:4 Batch:5/19 Loss:4.8496\n",
            "time: 19m 35s Epoch:4 Batch:6/19 Loss:4.9914\n",
            "time: 19m 54s Epoch:4 Batch:7/19 Loss:4.9819\n",
            "time: 20m 12s Epoch:4 Batch:8/19 Loss:4.9551\n",
            "time: 20m 32s Epoch:4 Batch:9/19 Loss:4.8514\n",
            "time: 20m 51s Epoch:4 Batch:10/19 Loss:4.8548\n",
            "time: 21m 9s Epoch:4 Batch:11/19 Loss:4.8518\n",
            "time: 21m 27s Epoch:4 Batch:12/19 Loss:4.7829\n",
            "time: 21m 45s Epoch:4 Batch:13/19 Loss:4.7268\n",
            "time: 22m 4s Epoch:4 Batch:14/19 Loss:4.7796\n",
            "time: 22m 22s Epoch:4 Batch:15/19 Loss:4.7461\n",
            "time: 22m 41s Epoch:4 Batch:16/19 Loss:4.8784\n",
            "time: 23m 0s Epoch:4 Batch:17/19 Loss:4.7546\n",
            "time: 23m 18s Epoch:4 Batch:18/19 Loss:4.7127\n",
            "time: 23m 37s Epoch:4 Batch:19/19 Loss:4.8386\n",
            "Epoch Completed. Epoch Loss:4.8514\n",
            "-----------------------------------------------------------------------------------------------------------------------------------------\n",
            "time: 23m 56s Epoch:5 Batch:0/19 Loss:4.7471\n",
            "time: 24m 15s Epoch:5 Batch:1/19 Loss:4.7259\n",
            "time: 24m 33s Epoch:5 Batch:2/19 Loss:4.7381\n",
            "time: 24m 52s Epoch:5 Batch:3/19 Loss:4.8579\n",
            "time: 25m 11s Epoch:5 Batch:4/19 Loss:4.7122\n",
            "time: 25m 29s Epoch:5 Batch:5/19 Loss:4.7441\n",
            "time: 25m 48s Epoch:5 Batch:6/19 Loss:4.786\n",
            "time: 26m 8s Epoch:5 Batch:7/19 Loss:4.7859\n",
            "time: 26m 26s Epoch:5 Batch:8/19 Loss:4.7551\n",
            "time: 26m 46s Epoch:5 Batch:9/19 Loss:4.7121\n",
            "time: 27m 5s Epoch:5 Batch:10/19 Loss:4.6909\n",
            "time: 27m 24s Epoch:5 Batch:11/19 Loss:4.6688\n",
            "time: 27m 42s Epoch:5 Batch:12/19 Loss:4.6113\n",
            "time: 28m 0s Epoch:5 Batch:13/19 Loss:4.5568\n",
            "time: 28m 19s Epoch:5 Batch:14/19 Loss:4.6205\n",
            "time: 28m 37s Epoch:5 Batch:15/19 Loss:4.6265\n",
            "time: 28m 56s Epoch:5 Batch:16/19 Loss:4.6569\n",
            "time: 29m 15s Epoch:5 Batch:17/19 Loss:4.6231\n",
            "time: 29m 33s Epoch:5 Batch:18/19 Loss:4.626\n",
            "time: 29m 53s Epoch:5 Batch:19/19 Loss:4.669\n",
            "Epoch Completed. Epoch Loss:4.6871\n",
            "-----------------------------------------------------------------------------------------------------------------------------------------\n",
            "time: 30m 12s Epoch:6 Batch:0/19 Loss:4.5705\n",
            "time: 30m 31s Epoch:6 Batch:1/19 Loss:4.5603\n",
            "time: 30m 49s Epoch:6 Batch:2/19 Loss:4.5452\n",
            "time: 31m 7s Epoch:6 Batch:3/19 Loss:4.6005\n",
            "time: 31m 26s Epoch:6 Batch:4/19 Loss:4.5772\n",
            "time: 31m 45s Epoch:6 Batch:5/19 Loss:4.5764\n",
            "time: 32m 4s Epoch:6 Batch:6/19 Loss:4.6794\n",
            "time: 32m 23s Epoch:6 Batch:7/19 Loss:4.6826\n",
            "time: 32m 42s Epoch:6 Batch:8/19 Loss:4.6664\n",
            "time: 33m 2s Epoch:6 Batch:9/19 Loss:4.6034\n",
            "time: 33m 21s Epoch:6 Batch:10/19 Loss:4.4926\n",
            "time: 33m 40s Epoch:6 Batch:11/19 Loss:4.5812\n",
            "time: 33m 58s Epoch:6 Batch:12/19 Loss:4.4954\n",
            "time: 34m 16s Epoch:6 Batch:13/19 Loss:4.4042\n",
            "time: 34m 35s Epoch:6 Batch:14/19 Loss:4.464\n",
            "time: 34m 53s Epoch:6 Batch:15/19 Loss:4.4961\n",
            "time: 35m 12s Epoch:6 Batch:16/19 Loss:4.4814\n",
            "time: 35m 31s Epoch:6 Batch:17/19 Loss:4.5509\n",
            "time: 35m 49s Epoch:6 Batch:18/19 Loss:4.4475\n",
            "time: 36m 9s Epoch:6 Batch:19/19 Loss:4.5372\n",
            "Epoch Completed. Epoch Loss:4.5473\n",
            "-----------------------------------------------------------------------------------------------------------------------------------------\n",
            "time: 36m 28s Epoch:7 Batch:0/19 Loss:4.4994\n",
            "time: 36m 47s Epoch:7 Batch:1/19 Loss:4.4604\n",
            "time: 37m 5s Epoch:7 Batch:2/19 Loss:4.4607\n",
            "time: 37m 24s Epoch:7 Batch:3/19 Loss:4.535\n",
            "time: 37m 42s Epoch:7 Batch:4/19 Loss:4.4259\n",
            "time: 38m 1s Epoch:7 Batch:5/19 Loss:4.5054\n",
            "time: 38m 20s Epoch:7 Batch:6/19 Loss:4.5649\n",
            "time: 38m 39s Epoch:7 Batch:7/19 Loss:4.5432\n",
            "time: 38m 58s Epoch:7 Batch:8/19 Loss:4.542\n",
            "time: 39m 18s Epoch:7 Batch:9/19 Loss:4.4801\n",
            "time: 39m 37s Epoch:7 Batch:10/19 Loss:4.3954\n",
            "time: 39m 56s Epoch:7 Batch:11/19 Loss:4.4753\n",
            "time: 40m 14s Epoch:7 Batch:12/19 Loss:4.3736\n",
            "time: 40m 32s Epoch:7 Batch:13/19 Loss:4.3211\n",
            "time: 40m 51s Epoch:7 Batch:14/19 Loss:4.3874\n",
            "time: 41m 9s Epoch:7 Batch:15/19 Loss:4.3966\n",
            "time: 41m 28s Epoch:7 Batch:16/19 Loss:4.3887\n",
            "time: 41m 47s Epoch:7 Batch:17/19 Loss:4.3732\n",
            "time: 42m 5s Epoch:7 Batch:18/19 Loss:4.4208\n",
            "time: 42m 25s Epoch:7 Batch:19/19 Loss:4.4469\n",
            "Epoch Completed. Epoch Loss:4.4426\n",
            "-----------------------------------------------------------------------------------------------------------------------------------------\n",
            "time: 42m 44s Epoch:8 Batch:0/19 Loss:4.3535\n",
            "time: 43m 3s Epoch:8 Batch:1/19 Loss:4.3713\n",
            "time: 43m 21s Epoch:8 Batch:2/19 Loss:4.3583\n",
            "time: 43m 40s Epoch:8 Batch:3/19 Loss:4.4328\n",
            "time: 43m 59s Epoch:8 Batch:4/19 Loss:4.3506\n",
            "time: 44m 18s Epoch:8 Batch:5/19 Loss:4.4176\n",
            "time: 44m 37s Epoch:8 Batch:6/19 Loss:4.4783\n",
            "time: 44m 56s Epoch:8 Batch:7/19 Loss:4.4385\n",
            "time: 45m 15s Epoch:8 Batch:8/19 Loss:4.4856\n",
            "time: 45m 35s Epoch:8 Batch:9/19 Loss:4.418\n",
            "time: 45m 54s Epoch:8 Batch:10/19 Loss:4.3403\n",
            "time: 46m 13s Epoch:8 Batch:11/19 Loss:4.3962\n",
            "time: 46m 31s Epoch:8 Batch:12/19 Loss:4.3056\n",
            "time: 46m 49s Epoch:8 Batch:13/19 Loss:4.2157\n",
            "time: 47m 8s Epoch:8 Batch:14/19 Loss:4.3111\n",
            "time: 47m 27s Epoch:8 Batch:15/19 Loss:4.3053\n",
            "time: 47m 45s Epoch:8 Batch:16/19 Loss:4.3319\n",
            "time: 48m 4s Epoch:8 Batch:17/19 Loss:4.2749\n",
            "time: 48m 23s Epoch:8 Batch:18/19 Loss:4.3181\n",
            "time: 48m 42s Epoch:8 Batch:19/19 Loss:4.359\n",
            "Epoch Completed. Epoch Loss:4.3566\n",
            "-----------------------------------------------------------------------------------------------------------------------------------------\n",
            "time: 49m 1s Epoch:9 Batch:0/19 Loss:4.2204\n",
            "time: 49m 20s Epoch:9 Batch:1/19 Loss:4.2912\n",
            "time: 49m 38s Epoch:9 Batch:2/19 Loss:4.2559\n",
            "time: 49m 57s Epoch:9 Batch:3/19 Loss:4.3496\n",
            "time: 50m 15s Epoch:9 Batch:4/19 Loss:4.2437\n",
            "time: 50m 34s Epoch:9 Batch:5/19 Loss:4.2756\n",
            "time: 50m 54s Epoch:9 Batch:6/19 Loss:4.3999\n",
            "time: 51m 13s Epoch:9 Batch:7/19 Loss:4.3756\n",
            "time: 51m 32s Epoch:9 Batch:8/19 Loss:4.4193\n",
            "time: 51m 52s Epoch:9 Batch:9/19 Loss:4.3667\n",
            "time: 52m 11s Epoch:9 Batch:10/19 Loss:4.242\n",
            "time: 52m 29s Epoch:9 Batch:11/19 Loss:4.2697\n",
            "time: 52m 48s Epoch:9 Batch:12/19 Loss:4.2007\n",
            "time: 53m 5s Epoch:9 Batch:13/19 Loss:4.1476\n",
            "time: 53m 24s Epoch:9 Batch:14/19 Loss:4.219\n",
            "time: 53m 43s Epoch:9 Batch:15/19 Loss:4.234\n",
            "time: 54m 2s Epoch:9 Batch:16/19 Loss:4.228\n",
            "time: 54m 20s Epoch:9 Batch:17/19 Loss:4.2455\n",
            "time: 54m 39s Epoch:9 Batch:18/19 Loss:4.288\n",
            "time: 54m 59s Epoch:9 Batch:19/19 Loss:4.3593\n",
            "Epoch Completed. Epoch Loss:4.279\n",
            "-----------------------------------------------------------------------------------------------------------------------------------------\n",
            "time: 55m 17s Epoch:10 Batch:0/19 Loss:4.1708\n",
            "time: 55m 37s Epoch:10 Batch:1/19 Loss:4.2637\n",
            "time: 55m 55s Epoch:10 Batch:2/19 Loss:4.2265\n",
            "time: 56m 13s Epoch:10 Batch:3/19 Loss:4.2411\n",
            "time: 56m 32s Epoch:10 Batch:4/19 Loss:4.2386\n",
            "time: 56m 51s Epoch:10 Batch:5/19 Loss:4.2519\n",
            "time: 57m 10s Epoch:10 Batch:6/19 Loss:4.3464\n",
            "time: 57m 30s Epoch:10 Batch:7/19 Loss:4.33\n",
            "time: 57m 49s Epoch:10 Batch:8/19 Loss:4.3318\n",
            "time: 58m 9s Epoch:10 Batch:9/19 Loss:4.296\n",
            "time: 58m 28s Epoch:10 Batch:10/19 Loss:4.1217\n",
            "time: 58m 46s Epoch:10 Batch:11/19 Loss:4.2492\n",
            "time: 59m 5s Epoch:10 Batch:12/19 Loss:4.1756\n",
            "time: 59m 22s Epoch:10 Batch:13/19 Loss:4.1036\n",
            "time: 59m 42s Epoch:10 Batch:14/19 Loss:4.1567\n",
            "time: 60m 0s Epoch:10 Batch:15/19 Loss:4.2119\n",
            "time: 60m 19s Epoch:10 Batch:16/19 Loss:4.2179\n",
            "time: 60m 38s Epoch:10 Batch:17/19 Loss:4.2075\n",
            "time: 60m 57s Epoch:10 Batch:18/19 Loss:4.2164\n",
            "time: 61m 16s Epoch:10 Batch:19/19 Loss:4.242\n",
            "Epoch Completed. Epoch Loss:4.2263\n",
            "-----------------------------------------------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpFnl9fgvlO2"
      },
      "source": [
        "  # saving the model for the purpose of re-training\n",
        "\n",
        "torch.save({\n",
        "            'modelA_state_dict': encoder.state_dict(),\n",
        "            'modelB_state_dict': decoder.state_dict(),\n",
        "            'optimizerA_state_dict': encoder_optim,\n",
        "            'optimizerB_state_dict': decoder_optim\n",
        "            }, '/content/drive/MyDrive/autoencoder.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4zyA9VqCFo_"
      },
      "source": [
        "def evaluate(encoder, decoder, input_tensor):\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "  output = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    input_length = input_tensor.size()[0]\n",
        "\n",
        "    encoder_h_t = encoder.initHidden()\n",
        "    encoder_c_t = encoder.initHidden()\n",
        "\n",
        "    for i in range(input_length):   # iterate the input length\n",
        "      encoder_output, encoder_h_t, encoder_c_t = encoder(input_tensor[i], encoder_h_t, encoder_c_t)\n",
        "\n",
        "    return encoder_output"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}